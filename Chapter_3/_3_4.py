# one-hot 编码 Y = [y_1,y_2...y_n]  有且仅有一个y_i = 1 且 other = 0
# softmax \frac {exp x_i} {\sum_j x_j}
# 

# practise 1 
# cross‐entropy loss 
# l(y , /hat y) = \sum_i^n - y_i \log_2 \hat y_i
# 一阶导数： \frac{\partial l} {\partial \hat_y} = - \sum_i \frac {y_i} {\hat y_i}
# 二阶导数： \sum_i \frac {y_i} {\hat y_i^2}
# softmax 特性： 
#   1. 所有元素（概率）都大于0;
#   2. 元素总和为1; 
#   3.完成了归一化,任何分布都可以抓换为一个概率分布，使用exp 使得置信度高的目标能够增加与置信度低的目标之间的差值
# 第二问 ： https://blog.csdn.net/qq_45732909/article/details/131189631 这位回答的比较好

# practise 2
# 1. 编码没啥问题，可以正确区分三种类别。
# 2. 使用one-hot 编码
# one-hot 编码优势（来自 Copilot）
'''
独热编码（One-Hot Encoding） 是将类别变量转换为机器学习算法易于利用的一种形式的过程。让我们探讨一下它的优势：

避免类别之间的大小关系：
标签编码（将类别映射为整数）假定类别值越高，该类别更好。这可能导致模型错误地认为某些类别之间存在大小关系。
独热编码将类别转换为二进制向量，避免了这个问题。每个类别都是独立的，没有大小关系。
适用于离散型特征：
独热编码为处理离散型特征提供了方法，扩充了特征属性。
它将类别变量转换为多个二进制特征，每个特征表示一个类别。
防止平均值计算错误：
如果在模型内部计算平均值（例如神经网络中的加权平均运算），标签编码可能导致错误的平均值。
独热编码避免了这个问题，确保模型预测不会受到大量误差。
保留类别信息：
独热编码不会丢失类别之间的顺序信息，因为每个类别都是一个独立的特征。
总之，独热编码是一种简单直接的方案，适用于处理离散型特征，避免了类别之间的大小关系，同时保留了类别信息。🌟
'''
